{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDw29pZPs2SM"
      },
      "source": [
        "This is an implementation of partial policy iteration with Q-networks.\n",
        "The algorithm is run on path selection task for symbolic execution.\n",
        "Each epoch we communicate with jar-file for data gathering and wandb for logging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "zCMPrusVs2SO"
      },
      "source": [
        "### Imports, meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BAP5ws3ofyyY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "\n",
        "from IPython.display import Javascript\n",
        "def resize_colab_cell():\n",
        "  display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 450})'))\n",
        "get_ipython().events.register('pre_run_cell', resize_colab_cell)\n",
        "\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import copy\n",
        "import inspect\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.onnx\n",
        "import json\n",
        "from tqdm import tqdm, trange\n",
        "from time import time\n",
        "import os\n",
        "import sklearn\n",
        "from sklearn import tree\n",
        "import math\n",
        "\n",
        "# !pip install wandb\n",
        "import wandb\n",
        "\n",
        "# !pip install onnx==1.12\n",
        "# import onnx\n",
        "\n",
        "# !pip install onnxruntime\n",
        "# import onnxruntime\n",
        "\n",
        "with open('../Game_env/jar_config.txt', 'w') as jar_config:\n",
        "    jar_config.write(json.dumps({\"algorithm\": \"TD\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "Tyw9mqV5s2SR"
      },
      "source": [
        "### Args (potentially immutable), login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "r4oNhj5DOBjw",
        "outputId": "844723d8-fced-44c5-90ba-2907105828ae",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 450})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrey_podivilov\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %%capture\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "z90K8IBjpUBu",
        "outputId": "cda45a33-90d7-4e4f-90aa-b43648ac4507",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 450})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 1024\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "td_gamma=0.99\n",
        "json_path = '../Data/current_dataset.json' # '/content/branching_small.json'\n",
        "use_TD = True\n",
        "use_MC = 1 - use_TD\n",
        "use_fork_discount = False\n",
        "\n",
        "jar_command = '/home/st-andrey-podivilov/java16/usr/lib/jvm/bellsoft-java16-amd64/bin/java -jar ../Game_env/usvm-jvm/build/libs/usvm-jvm-new.jar ../Game_env/jar_config.txt > ../Game_env/jar_log.txt'\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "MmYgxqVAs2ST"
      },
      "source": [
        "### Models, modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "abh-k_ytgHWT",
        "outputId": "d7afca61-ea0b-4f4d-d851-95c77b7b2cc1",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 450})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class FFM_layer(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Why Not?\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "      super().__init__()\n",
        "      assert input_dim%2 == 0, 'even input_dim is more convenient'\n",
        "      self.fourier_matrix = torch.nn.Linear(input_dim, int(input_dim), bias=False)\n",
        "      nn.init.normal_(\n",
        "          self.fourier_matrix.weight,\n",
        "          std=1/np.sqrt(input_dim),\n",
        "      )\n",
        "      self.fourier_matrix.weight.requires_grad_(False)\n",
        "\n",
        "    def forward(self, x):\n",
        "      pre = x # self.fourier_matrix(x)\n",
        "      s = torch.sin(pre)\n",
        "      c = torch.cos(pre)\n",
        "      return torch.cat([x,s,c], dim=-1)\n",
        "\n",
        "def get_mlp_setup(use_FFM=False):\n",
        "    mlp = nn.Sequential(\n",
        "        nn.LazyLinear(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,256),\n",
        "        nn.LayerNorm(256),\n",
        "        FFM_layer(256) if use_FFM else nn.Identity(),\n",
        "        nn.LazyLinear(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,1),\n",
        "    ).to(device)\n",
        "    mlp_opt = torch.optim.AdamW(mlp.parameters(), lr=3e-4, weight_decay=0.1, betas=(0.9, 0.99))\n",
        "    return mlp, mlp_opt\n",
        "\n",
        "# to check features' strength\n",
        "r_tree = tree.DecisionTreeRegressor(max_depth=1000, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "rGo0ehMFs2SU"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ov4TPknTgHSy",
        "outputId": "56487d39-1ae0-4e7a-a179-728941e4d9bf",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 450})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class Trajectories:\n",
        "  \"\"\"\n",
        "  Contains all kinds of data in a form of tensor.\n",
        "  train_tensors are raw and derivative features of visited states.\n",
        "  Action and state embeddings are effectively the same, fyi.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               path=json_path,\n",
        "               td_gamma=td_gamma,\n",
        "               eval_condition = (lambda x: x%hash_modulo == 0),\n",
        "              ):\n",
        "    self.eval_condition = eval_condition\n",
        "    self.td_gamma = td_gamma\n",
        "    self.j_file = json.load(open(path))\n",
        "    self.feature_names = self.j_file['scheme'][0]\n",
        "    self.feature_names2ids = {self.feature_names[i]:i for i in range(len(self.feature_names))}\n",
        "    self.train_tensors = self.j2torch(self.j_file) #sarsa list of 5 tensors of length n_states-n_trajectories f, f_n, r, R, is_last\n",
        "    self.n_sarsa_pairs = self.n_sarsa_pairs()\n",
        "\n",
        "  def j2torch(self, j_file):\n",
        "    \"\"\"\n",
        "    transforms json to data tensors\n",
        "    \"\"\"\n",
        "    features, features_next, rewards, Returns, is_last = [], [], [], [], []\n",
        "    chosenStId_idx = self.j_file['scheme'].index('chosenStateId')\n",
        "    rewards_idx = self.j_file['scheme'].index('reward')\n",
        "\n",
        "    for tr in self.j_file['paths']:\n",
        "      if self.eval_condition(tr[0]):\n",
        "        continue\n",
        "      tr = tr[1]\n",
        "      tr_rewards = [tr[i][rewards_idx] for i in range(len(tr))]\n",
        "      rewards += tr_rewards\n",
        "      do_discount = torch.Tensor([1]*len(tr))\n",
        "      if use_fork_discount:\n",
        "        is_cfg_fork_idx = self.j_file['scheme'].index('is_cfg_fork')\n",
        "        do_discount = [tr[i][is_cfg_fork_idx] for i in range(len(tr))]\n",
        "      tr_Returns = self.tr_rewards_to_returns(tr_rewards, do_discount)\n",
        "      Returns += tr_Returns\n",
        "\n",
        "      tr_features = [tr[i][0][tr[i][chosenStId_idx]] for i in range(len(tr))]\n",
        "      is_last += [0]*(len(tr_features)-1) + [1]\n",
        "      features += tr_features\n",
        "      features_next += tr_features[1:] + [[0]*len(tr_features[0])]\n",
        "    rewards = torch.Tensor(rewards).to(device)\n",
        "    features = torch.Tensor(features).to(device)\n",
        "    features_next = torch.Tensor(features_next).to(device)\n",
        "    Returns = torch.Tensor(Returns).to(device)\n",
        "    is_last = torch.Tensor(is_last).to(device)\n",
        "    return [features, features_next, rewards, Returns, is_last]\n",
        "\n",
        "  def tr_rewards_to_returns(self, tr_rewards, do_discount):\n",
        "    tr_R = [0]*(len(tr_rewards)-1) + [tr_rewards[-1]]\n",
        "    for i in range(len(tr_rewards)-2, -1, -1):\n",
        "        tr_R[i] = tr_rewards[i] + (self.td_gamma**do_discount[i]) * tr_R[i+1]\n",
        "    return tr_R\n",
        "\n",
        "  def n_sarsa_pairs(self):\n",
        "     return len(self.train_tensors[0])\n",
        "\n",
        "  def sample_batch(self, n=batch_size):\n",
        "    ids = torch.tensor(random.choice(self.n_sarsa_pairs, size=n)).long()\n",
        "    sampled = [t[ids] for t in self.train_tensors]\n",
        "    return sampled\n",
        "\n",
        "  def update_data_on_path(self, path, model):\n",
        "    \"\"\"\n",
        "    Communication with jar file on a server.\n",
        "    \"\"\"\n",
        "    x = torch.randn(1, self.train_tensors[0][0].shape[0], requires_grad=True).to(device)\n",
        "    torch_model = model.eval()\n",
        "    torch_out = torch_model(x)\n",
        "\n",
        "    torch.onnx.export(torch_model,\n",
        "                      x,\n",
        "                      '../Game_env/model.onnx',\n",
        "                      opset_version=13,\n",
        "                      export_params=True,\n",
        "                      input_names = ['input'],   # the model's input names\n",
        "                      output_names = ['output'],\n",
        "                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
        "                                    'output' : {0 : 'batch_size'},\n",
        "                                    },\n",
        "                      )\n",
        "\n",
        "    os.system(jar_command)\n",
        "\n",
        "  def evaluate_data(self,\n",
        "           factors=torch.Tensor([1, 0.99, 0.95]),\n",
        "           eval_condition = None,\n",
        "           verbose=True,\n",
        "           wandb_prefix = 'val',\n",
        "           ):\n",
        "    if eval_condition is None:\n",
        "        eval_condition = self.eval_condition\n",
        "    rewards_idx = self.j_file['scheme'].index('reward')\n",
        "    for f in factors:\n",
        "      size = 0\n",
        "      tr_lengths = []\n",
        "      trs_R = []\n",
        "      for tr in self.j_file['paths']:\n",
        "        if not eval_condition(tr[0]):\n",
        "          continue\n",
        "        tr=tr[1]\n",
        "        size += len(tr)\n",
        "        tr_lengths += [len(tr)]\n",
        "        tr_rewards = [tr[i][rewards_idx] for i in range(len(tr))]\n",
        "        trs_R += [0]\n",
        "        do_discount = torch.Tensor([1]*len(tr))\n",
        "        if use_fork_discount:\n",
        "          is_cfg_fork_idx = self.j_file['scheme'].index('is_cfg_fork')\n",
        "          do_discount = [tr[i][is_cfg_fork_idx] for i in range(len(tr))]\n",
        "        for i in range(len(tr_rewards)-1, -1, -1):\n",
        "          trs_R[-1] = tr_rewards[i] + (f ** do_discount[i]) * trs_R[-1]\n",
        "      log = {}\n",
        "      log[f'{wandb_prefix} size'] = size\n",
        "      log[f'{wandb_prefix}_eval/mean {f:.2f} discount '] = torch.Tensor(trs_R).mean()\n",
        "      log[f'{wandb_prefix}_eval/median {f:.2f} discount '] = torch.Tensor(trs_R).median()\n",
        "      # log[f'{wandb_prefix}_eval/95_quanile {f:.2f} discount'] = torch.Tensor(trs_R).quantile(q=0.95)\n",
        "      log[f'{wandb_prefix} Return by trjs {f:.2f} hist (previous)'] = wandb.Histogram(np_histogram=np.histogram(trs_R, bins=50, ))\n",
        "#       log[f'{wandb_prefix} lengths hist'] = wandb.Histogram(np_histogram=np.histogram(tr_lengths, bins=30, ))\n",
        "      if verbose:\n",
        "          wandb.log(log.copy())\n",
        "      log['Returns'] = trs_R\n",
        "      log[f'{wandb_prefix} lengths'] = tr_lengths\n",
        "    return log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "S4soqLMvs2SX"
      },
      "source": [
        "### Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TW_VkrRNJeaV",
        "outputId": "344f0c51-bc82-473e-db51-e87400471bc4",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 450})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class Logger:\n",
        "  \"\"\"\n",
        "  Supporting class, to be expanded.\n",
        "  Stores logging methods and relevant data.\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "      self,\n",
        "      NN_setup,\n",
        "      batch_size=batch_size,\n",
        "      between_logs = 0,\n",
        "  ):\n",
        "    self.model = NN_setup['model']\n",
        "    self.optimizer = NN_setup['optimizer']\n",
        "    self.grad = None\n",
        "    self.weight = None\n",
        "    self.running_grad_mean = torch.zeros(len([p for p in self.model.parameters() if p.requires_grad])).to(device)\n",
        "    self.running_grad2_mean = torch.zeros(len([p for p in self.model.parameters() if p.requires_grad])).to(device)\n",
        "    self.log_gamma = torch.tensor(0.95).to(device)\n",
        "    self.between_logs = between_logs\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def link_model(self,):\n",
        "    self.grad = [p.grad.detach() for p in self.model.parameters() if p.requires_grad]\n",
        "    self.weight = [p.detach() for p in self.model.parameters()]\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def list_norm(self, l, p=2):\n",
        "    n = 0\n",
        "    for t in l:\n",
        "      n += t.detach().norm(p) ** p\n",
        "    return n.item() ** (1/p)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def list_cos_dist(self, a, b):\n",
        "    a_norm = self.list_norm(a, 2)\n",
        "    b_norm = self.list_norm(b, 2)\n",
        "    product = sum([torch.dot(torch.flatten(a[i]), torch.flatten(b[i])).item() for i in range(len(a))])\n",
        "    return product/(a_norm*b_norm)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def on_list(self, a, b, operation):\n",
        "    assert len(a) == len(b), 'lists lengths differ'\n",
        "    return [operation(a[i], b[i]) for i in range(len(a))]\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def running_mean(self, a, b):\n",
        "    return [a[i].mul(self.log_gamma) + b[i].mul(1 - self.log_gamma) for i in range(len(a))]\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self):\n",
        "    self.running_grad_mean = self.running_mean(self.running_grad_mean, self.grad)\n",
        "    self.running_grad2_mean = self.running_mean(self.running_grad2_mean, [g**2 for g in self.grad])\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def grad_stdev(self):\n",
        "    dev = [self.running_grad2_mean[i] - m**2 for i, m in enumerate(self.running_grad_mean)]\n",
        "    return [torch.sqrt(torch.maximum(torch.tensor(0), d)) for d in dev]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "Fq0cbk7Cs2SY"
      },
      "source": [
        "### Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NsLLouOFgHQZ",
        "outputId": "fcc2f3f8-9c56-4071-ab7d-d944048bdc62",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 450})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class NN_Trainer:\n",
        "  def __init__(\n",
        "      self,\n",
        "      NN_setup,\n",
        "      logger=None,\n",
        "      trajectories=None,\n",
        "      batch_size=batch_size,\n",
        "      n_batches=1000,\n",
        "      target_update_steps = 20,\n",
        "      td_gamma=td_gamma,\n",
        "      ):\n",
        "    self.n_batches = n_batches\n",
        "    self.batch_number = -1\n",
        "    self.td_gamma = td_gamma\n",
        "    self.model = NN_setup['model'].train()\n",
        "    self.target_model = copy.deepcopy(self.model).eval()\n",
        "    self.optimizer = NN_setup['optimizer']\n",
        "    self.trajectories = trajectories\n",
        "    self.batch_size = batch_size\n",
        "    self.target_update_steps = target_update_steps\n",
        "    self.logger = logger\n",
        "    self.log = {}\n",
        "\n",
        "  def MC_loss(self,\n",
        "              features,\n",
        "              Returns,\n",
        "              ):\n",
        "    \"\"\"\n",
        "    Optional loss computed from Monte Carlo estimates\n",
        "    \"\"\"\n",
        "    q = self.model(features)\n",
        "    loss = torch.mean((q.squeeze() - Returns)**2)\n",
        "    if self.batch_number % (self.logger.between_logs+1) == 0:\n",
        "      self.log['Q-function mean'] = torch.mean(q.detach()).item()\n",
        "      self.log['Q-function stdev'] = torch.std(q.detach()).item()\n",
        "      hist = wandb.Histogram(np_histogram=np.histogram(q.detach().to('cpu'), bins=30, ))\n",
        "      self.log['Q-function hist'] = hist\n",
        "      hist = wandb.Histogram(np_histogram=np.histogram(Returns.detach().to('cpu'), bins=30, ))\n",
        "      self.log['MC Return estimate hist'] = hist\n",
        "      self.log['Return mean'] = Returns.mean().item()\n",
        "      self.log['Return std'] = Returns.std().item()\n",
        "    return torch.sqrt(loss)\n",
        "\n",
        "  def td1_loss(self,\n",
        "               features,\n",
        "               features_next,\n",
        "               rewards,\n",
        "               is_last,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Temporal difference loss\n",
        "    \"\"\"\n",
        "    q = self.model(features)\n",
        "    with torch.no_grad():\n",
        "      q_next = self.target_model(features_next).detach()\n",
        "    TD = q.squeeze() - (rewards + q_next.squeeze() * self.td_gamma * (1-is_last))\n",
        "    loss = torch.mean(TD**2)\n",
        "\n",
        "    if self.batch_number % (self.logger.between_logs+1) == 0:\n",
        "      self.log['Q-function mean'] = torch.mean(q.detach()).item()\n",
        "      self.log['Q-function stdev'] = torch.std(q.detach()).item()\n",
        "      hist = wandb.Histogram(np_histogram=np.histogram(q.detach().to('cpu'), bins=30, ))\n",
        "      self.log['Q-function hist'] = hist\n",
        "      hist = wandb.Histogram(np_histogram=np.histogram(TD.detach().to('cpu'), bins=80, ))\n",
        "      self.log['TD hist'] = hist\n",
        "    return loss /10\n",
        "\n",
        "  def learn_q(self, ):\n",
        "    \"\"\"\n",
        "    Approximates Q-function of previous policy by iterating over collected data.\n",
        "    Q-function is used later on to update data gathering policy in an argmax fasion.\n",
        "    \"\"\"\n",
        "    logger=self.logger\n",
        "    for i in trange(self.n_batches):\n",
        "      self.batch_number = i\n",
        "      if self.batch_number % self.target_update_steps == 0:\n",
        "        self.target_model = copy.deepcopy(self.model).eval()\n",
        "      f, f_next, r, Returns, is_last = self.trajectories.sample_batch(self.batch_size)\n",
        "      if use_TD:\n",
        "        loss = self.td1_loss(f, f_next, r, is_last)\n",
        "      if use_MC:\n",
        "        loss = self.MC_loss(f, Returns)\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(self.model.parameters(), 20)\n",
        "      self.optimizer.step()\n",
        "\n",
        "      logger.link_model()\n",
        "      logger.step()\n",
        "\n",
        "      if self.batch_number % (logger.between_logs+1) == 0:\n",
        "        self.log.update({\n",
        "            # 'batch_number': self.batch_number,\n",
        "            'loss': loss.item(),\n",
        "            'reward mean': torch.mean(r).item(),\n",
        "            'grad L2': logger.list_norm(logger.grad, 2),\n",
        "            'weight L2': logger.list_norm(logger.weight, 2),\n",
        "            # 'grad_stdev L2': logger.list_norm(logger.grad_stdev(), 2),\n",
        "            # 'some_feature_mean': features[:, self.trajectories.feature_names2ids['some_feature']].mean()\n",
        "        })\n",
        "        wandb.log(self.log)\n",
        "        self.log = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ObFvmiz_s2SZ"
      },
      "source": [
        "### Procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "Bi2uaButgHOM",
        "outputId": "ac74fdf1-0fb8-4613-b567-6b8b14dec89b",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 450})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "246 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial data gathering time: 200.15660095214844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/st-andrey-podivilov/.conda/envs/env1/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.7 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/st-andrey-podivilov/Notebooks/wandb/run-20230727_174557-o3642d2f</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/andrey_podivilov/PS%20PPO/runs/o3642d2f' target=\"_blank\">TD, val %5==0</a></strong> to <a href='https://wandb.ai/andrey_podivilov/PS%20PPO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/andrey_podivilov/PS%20PPO' target=\"_blank\">https://wandb.ai/andrey_podivilov/PS%20PPO</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/andrey_podivilov/PS%20PPO/runs/o3642d2f' target=\"_blank\">https://wandb.ai/andrey_podivilov/PS%20PPO/runs/o3642d2f</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:02<00:00, 188.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "247 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data gathering time:  214.1245400905609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 269.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "246 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data gathering time:  191.606671333313\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 267.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "265 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data gathering time:  211.77847242355347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 258.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "269 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data gathering time:  186.7319459915161\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 268.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "248 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data gathering time:  180.3336684703827\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:02<00:00, 238.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "262 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data gathering time:  238.157940864563\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 266.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "257 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data gathering time:  184.48891520500183\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 272.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "253 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data gathering time:  187.94905495643616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 278.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "256 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data gathering time:  199.6092565059662\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 278.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "247 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data gathering time:  194.69418382644653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:01<00:00, 278.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "249 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n",
            "Aborted (core dumped)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data gathering time:  128.94689965248108\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 491/500 [00:01<00:00, 277.61it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m trajectories\u001b[38;5;241m.\u001b[39mevaluate_data()    \n\u001b[1;32m     43\u001b[0m trajectories\u001b[38;5;241m.\u001b[39mevaluate_data(wandb_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     44\u001b[0m                          eval_condition\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;129;01mnot\u001b[39;00m trajectories\u001b[38;5;241m.\u001b[39meval_condition(x)),\n\u001b[1;32m     45\u001b[0m                          )\n\u001b[0;32m---> 46\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[1;32m     49\u001b[0m         })\n\u001b[1;32m     51\u001b[0m time_before \u001b[38;5;241m=\u001b[39m time()\n",
            "Cell \u001b[0;32mIn[7], line 81\u001b[0m, in \u001b[0;36mNN_Trainer.learn_q\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     80\u001b[0m logger\u001b[38;5;241m.\u001b[39mlink_model()\n\u001b[0;32m---> 81\u001b[0m \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_number \u001b[38;5;241m%\u001b[39m (logger\u001b[38;5;241m.\u001b[39mbetween_logs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     84\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m     85\u001b[0m       \u001b[38;5;66;03m# 'batch_number': self.batch_number,\u001b[39;00m\n\u001b[1;32m     86\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[38;5;66;03m# 'some_feature_mean': features[:, self.trajectories.feature_names2ids['some_feature']].mean()\u001b[39;00m\n\u001b[1;32m     92\u001b[0m   })\n",
            "File \u001b[0;32m~/.conda/envs/env1/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[6], line 50\u001b[0m, in \u001b[0;36mLogger.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     49\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_grad_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_grad_mean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m---> 50\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_grad2_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_grad2_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.conda/envs/env1/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[6], line 45\u001b[0m, in \u001b[0;36mLogger.running_mean\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrunning_mean\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, b):\n\u001b[0;32m---> 45\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [a[i]\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_gamma) \u001b[38;5;241m+\u001b[39m b[i]\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_gamma) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(a))]\n",
            "Cell \u001b[0;32mIn[6], line 45\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrunning_mean\u001b[39m(\u001b[38;5;28mself\u001b[39m, a, b):\n\u001b[0;32m---> 45\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [a[i]\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_gamma) \u001b[38;5;241m+\u001b[39m b[i]\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_gamma) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(a))]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# We start with \"BFS\" heuristic as policy (not to be confused with naive BFS)\n",
        "time_before = time()\n",
        "os.system('rm -f ../Game_env/model.onnx')\n",
        "os.system(jar_command)\n",
        "print('initial data gathering time:', time() - time_before)\n",
        "\n",
        "epochs = 30\n",
        "eval_conditions=[(lambda x: x%5==0)]\n",
        "td_gammas = [0.99]\n",
        "\n",
        "for (eval_condition, td_gamma) in zip(eval_conditions, td_gammas):\n",
        "    mlp, mlp_opt = get_mlp_setup(use_FFM=True)\n",
        "    run = wandb.init(\n",
        "          project=\"PS TD\",\n",
        "          name=f'TD, val {inspect.getsourcelines(eval_condition)[0][0].split(\"x\")[-1].split(\")\")[0]}',\n",
        "          config={\n",
        "              'algorithm': 'partial policy iteration q-function',\n",
        "              'model': 'mlp',\n",
        "          }\n",
        "    )\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        trajectories = Trajectories(json_path,\n",
        "                                    eval_condition=eval_condition,\n",
        "                                    td_gamma=td_gamma,\n",
        "                                   )\n",
        "        logger = Logger(NN_setup={'model': mlp, 'optimizer': mlp_opt},\n",
        "                        batch_size=batch_size,\n",
        "                        between_logs = 10,\n",
        "                       )\n",
        "        trainer = NN_Trainer(\n",
        "          {'model': mlp, 'optimizer': mlp_opt},\n",
        "          logger=logger,\n",
        "          trajectories=trajectories,\n",
        "          batch_size=batch_size,\n",
        "          n_batches=500,\n",
        "          td_gamma=td_gamma,\n",
        "          )\n",
        "\n",
        "        trajectories.evaluate_data()\n",
        "        trajectories.evaluate_data(wandb_prefix='train',\n",
        "                                 eval_condition=(lambda x: not trajectories.eval_condition(x)),\n",
        "                                 )\n",
        "        trainer.learn_q()\n",
        "\n",
        "        wandb.log({'epoch': epoch,\n",
        "                })\n",
        "\n",
        "        time_before = time()\n",
        "        trajectories.update_data_on_path(path='../Data/current_dataset.json', model=trainer.model)\n",
        "        print('Data gathering time: ', time()-time_before)\n",
        "\n",
        "    trajectories.update_data_on_path(path='../Data/current_dataset.json', model=trainer.model)\n",
        "    trajectories.evaluate_data()\n",
        "\n",
        "    checkpoint = {\n",
        "    'model': mlp,\n",
        "    }\n",
        "    # torch.save(checkpoint, os.path.join(wandb.run.dir, f'mlp for TD multistep'))\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDZ1NGUEs2SZ"
      },
      "outputs": [],
      "source": [
        "# exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mwkUX5ns2Sa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqP3rMwPs2Sa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "zCMPrusVs2SO",
        "Tyw9mqV5s2SR",
        "MmYgxqVAs2ST",
        "rGo0ehMFs2SU",
        "S4soqLMvs2SX",
        "Fq0cbk7Cs2SY"
      ]
    },
    "kernelspec": {
      "display_name": "env1_new",
      "language": "python",
      "name": "env1_new"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}