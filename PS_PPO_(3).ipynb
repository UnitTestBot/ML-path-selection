{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyUgqtNPsWCV"
      },
      "source": [
        "This is an implementation of PPO-clip for path selection for symbolic execution.\n",
        "Each epoch we communicate with jar-file for data gathering and wandb for logging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6GWfxrs9gFR",
        "tags": []
      },
      "source": [
        "### Imports, meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAP5ws3ofyyY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "\n",
        "from IPython.display import Javascript\n",
        "def resize_colab_cell():\n",
        "  display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 600})'))\n",
        "get_ipython().events.register('pre_run_cell', resize_colab_cell)\n",
        "\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import copy\n",
        "import inspect\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.onnx\n",
        "import json\n",
        "from tqdm import tqdm, trange\n",
        "from time import time\n",
        "import os\n",
        "import sklearn\n",
        "from sklearn import tree\n",
        "import math\n",
        "\n",
        "# !pip install wandb\n",
        "import wandb\n",
        "\n",
        "# !pip install onnx==1.12\n",
        "# import onnx\n",
        "\n",
        "with open('../Game_env/jar_config.txt', 'w') as jar_config:\n",
        "    jar_config.write(json.dumps({\"algorithm\": \"PPO\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJbCP9HE9gFV",
        "tags": []
      },
      "source": [
        "### Args (potentially immutable), login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "r4oNhj5DOBjw",
        "outputId": "844723d8-fced-44c5-90ba-2907105828ae",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrey_podivilov\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %%capture\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "z90K8IBjpUBu",
        "outputId": "cda45a33-90d7-4e4f-90aa-b43648ac4507",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 4096\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "td_gamma=0.99\n",
        "json_path = '../Data/current_dataset.json'\n",
        "use_fork_discount = False\n",
        "\n",
        "jar_command = '/home/st-andrey-podivilov/java16/usr/lib/jvm/bellsoft-java16-amd64/bin/java -jar ../Game_env/usvm-jvm/build/libs/usvm-jvm-new.jar > ../Game_env/jar_log.txt'\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndNrbEp79gFX",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Models, modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "abh-k_ytgHWT",
        "outputId": "d7afca61-ea0b-4f4d-d851-95c77b7b2cc1",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class FFM_layer(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Why Not?\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "      super().__init__()\n",
        "      assert input_dim%2 == 0, 'even input_dim is more convenient'\n",
        "      self.fourier_matrix = torch.nn.Linear(input_dim, int(input_dim), bias=False)\n",
        "      nn.init.normal_(\n",
        "          self.fourier_matrix.weight,\n",
        "          std=1/np.sqrt(input_dim),\n",
        "      )\n",
        "      self.fourier_matrix.weight.requires_grad_(False)\n",
        "\n",
        "    def forward(self, x):\n",
        "      pre = x # self.fourier_matrix(x)\n",
        "      s = torch.sin(pre)\n",
        "      c = torch.cos(pre)\n",
        "      return torch.cat([x,s,c], dim=-1)\n",
        "\n",
        "def get_model_setup(use_FFM=False,\n",
        "                    lr = 3e-4,\n",
        "                    wd=0.1,\n",
        "                    ):\n",
        "    mlp = nn.Sequential(\n",
        "        nn.LazyLinear(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,256),\n",
        "        nn.LayerNorm(256),\n",
        "        FFM_layer(256) if use_FFM else nn.Identity(),\n",
        "        nn.LazyLinear(1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(1024,1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(1024,1),\n",
        "    ).to(device)\n",
        "    mlp_opt = torch.optim.AdamW(mlp.parameters(), lr=lr, weight_decay=0.1, betas=(0.9, 0.99))\n",
        "    return mlp, mlp_opt\n",
        "\n",
        "# to check features' strength\n",
        "r_tree = tree.DecisionTreeRegressor(max_depth=1000, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBhLnmrnFDAu",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ov4TPknTgHSy",
        "outputId": "56487d39-1ae0-4e7a-a179-728941e4d9bf",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class Trajectories:\n",
        "  \"\"\"\n",
        "  Contains all kinds of data in a form of tensor.\n",
        "  realized_tensors are raw and derivative features of visited states.\n",
        "  queues is a list of each states' actions features.\n",
        "  Action and state embeddings are effectively the same, fyi.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               path=json_path,\n",
        "               td_gamma=td_gamma,\n",
        "               eval_condition = (lambda x: x%5==0),\n",
        "              ):\n",
        "    self.eval_condition = eval_condition\n",
        "    self.td_gamma = td_gamma\n",
        "    self.j_file = json.load(open(path))\n",
        "    self.feature_names = self.j_file['scheme'][0]\n",
        "    self.feature_names2ids = {self.feature_names[i]:i for i in range(len(self.feature_names))}\n",
        "    self.realized_tensors, self.queues = self.j2torch(self.j_file) #list of 5 tensors f, f_n, r, R, is_last + list of queue tensors\n",
        "    self.n_states = self.n_states()\n",
        "\n",
        "\n",
        "  def j2torch(self, j_file):\n",
        "    \"\"\"\n",
        "    transforms json to data tensors\n",
        "    \"\"\"\n",
        "    features, features_next, rewards, Returns, is_last, queues = [], [], [], [], [], []\n",
        "    chosenStId_idx = self.j_file['scheme'].index('chosenStateId')\n",
        "    rewards_idx = self.j_file['scheme'].index('reward')\n",
        "\n",
        "    for tr in self.j_file['paths']:\n",
        "      if self.eval_condition(tr[0]):\n",
        "        continue\n",
        "      tr = tr[1]\n",
        "      tr_rewards = [tr[i][rewards_idx] for i in range(len(tr))]\n",
        "      rewards += tr_rewards\n",
        "      do_discount = torch.Tensor([1]*len(tr))\n",
        "      if use_fork_discount:\n",
        "        is_cfg_fork_idx = self.j_file['scheme'].index('is_cfg_fork')\n",
        "        do_discount = [tr[i][is_cfg_fork_idx] for i in range(len(tr))]\n",
        "      tr_Returns = self.tr_rewards_to_returns(tr_rewards, do_discount)\n",
        "      Returns += tr_Returns\n",
        "\n",
        "      tr_features = [tr[i][0][tr[i][chosenStId_idx]] for i in range(len(tr))]\n",
        "      is_last += [0]*(len(tr_features)-1) + [1]\n",
        "      features += tr_features\n",
        "      features_next += tr_features[1:] + [[-1]*len(tr_features[0])]\n",
        "\n",
        "      tr_queues = [torch.Tensor(tr[i][0]) for i in range(len(tr))][1:] + [torch.zeros_like(torch.Tensor([tr[0][0][0]]))]\n",
        "      queues += tr_queues\n",
        "    rewards = torch.Tensor(rewards).to(device)\n",
        "    features = torch.Tensor(features).to(device)\n",
        "    features_next = torch.Tensor(features_next).to(device)\n",
        "    Returns = torch.Tensor(Returns).to(device)\n",
        "    is_last = torch.Tensor(is_last).to(device)\n",
        "    return [features, features_next, rewards, Returns, is_last], queues\n",
        "\n",
        "  def n_states(self):\n",
        "    return len(self.realized_tensors[-1])\n",
        "\n",
        "  def get_properties(self):\n",
        "    longest_queue_ids = np.argmax(np.array([q.shape[0] for q in self.queues]))\n",
        "    prop = {\n",
        "        'queue max length, idx': (self.queues[longest_queue_ids].shape[0], longest_queue_ids),\n",
        "        'total number of states': len(self.realized_tensors[-1]),\n",
        "        'number of traj-s': len(self.j_file['paths']),\n",
        "        'number of validation traj-s': sum([self.eval_condition(tr[0]) for tr in self.j_file['paths']]),\n",
        "        }\n",
        "    # assert self.queues[longest_queue_ids].shape[0] < batch_size/5, 'not that i.i.d sampling, better rewrite?'\n",
        "    return prop\n",
        "\n",
        "  def tr_rewards_to_returns(self, tr_rewards, do_discount):\n",
        "    tr_R = [0]*(len(tr_rewards)-1) + [tr_rewards[-1]]\n",
        "    for i in range(len(tr_rewards)-2, -1, -1):\n",
        "        tr_R[i] = tr_rewards[i] + (self.td_gamma**do_discount[i]) * tr_R[i+1]\n",
        "    return tr_R\n",
        "\n",
        "  def sample_batch(self, batch_size=batch_size):\n",
        "    \"\"\"\n",
        "    Queues are diverse in length, but we want them in one batch still.\n",
        "    \"\"\"\n",
        "    ids_to_try = torch.tensor(random.choice(self.n_states, size=batch_size)).long()\n",
        "    ids = []\n",
        "    sampled_queues = []\n",
        "    bins = [0]\n",
        "    accum_len = 0\n",
        "    # naive\n",
        "    for idx in ids_to_try:\n",
        "      cur_len = self.queues[idx.item()].shape[0]\n",
        "      if accum_len + cur_len > batch_size:\n",
        "        break\n",
        "      accum_len += cur_len\n",
        "      bins += [accum_len]\n",
        "      ids += [idx]\n",
        "      sampled_queues += [self.queues[idx.item()]]\n",
        "    ids = torch.LongTensor(ids).to(device)\n",
        "    bins = torch.LongTensor(bins).to(device)\n",
        "    sampled_queues = torch.cat(sampled_queues).to(device)\n",
        "\n",
        "    sampled_realized = [t[ids] for t in self.realized_tensors]\n",
        "    return *sampled_realized, sampled_queues, bins\n",
        "\n",
        "  def update_data_on_path(self, path, model):\n",
        "    \"\"\"\n",
        "    Communication with jar file on a server.\n",
        "    \"\"\"\n",
        "    x = torch.randn(1, self.realized_tensors[0][0].shape[0], requires_grad=True).to(device)\n",
        "    torch_model = model.eval()\n",
        "    torch_out = torch_model(x)\n",
        "    torch.onnx.export(torch_model,\n",
        "                      x,\n",
        "                      '../Game_env/model.onnx',\n",
        "                      opset_version=13,\n",
        "                      export_params=True,\n",
        "                      input_names = ['input'],   # the model's input names\n",
        "                      output_names = ['output'],\n",
        "                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
        "                                    'output' : {0 : 'batch_size'},\n",
        "                                    },\n",
        "                      )\n",
        "\n",
        "    os.system(jar_command)\n",
        "\n",
        "  def evaluate_data(self,\n",
        "           factors=torch.Tensor([1, 0.99, 0.95]),\n",
        "           eval_condition = None,\n",
        "           verbose=True,\n",
        "           wandb_prefix = 'val',\n",
        "           ):\n",
        "    if eval_condition is None:\n",
        "        eval_condition = self.eval_condition\n",
        "    rewards_idx = self.j_file['scheme'].index('reward')\n",
        "    for f in factors:\n",
        "      size = 0\n",
        "      tr_lengths = []\n",
        "      trs_R = []\n",
        "      for tr in self.j_file['paths']:\n",
        "        if not eval_condition(tr[0]):\n",
        "          continue\n",
        "        tr=tr[1]\n",
        "        size += len(tr)\n",
        "        tr_lengths += [len(tr)]\n",
        "        tr_rewards = [tr[i][rewards_idx] for i in range(len(tr))]\n",
        "        trs_R += [0]\n",
        "        do_discount = torch.Tensor([1]*len(tr))\n",
        "        if use_fork_discount:\n",
        "          is_cfg_fork_idx = self.j_file['scheme'].index('is_cfg_fork')\n",
        "          do_discount = [tr[i][is_cfg_fork_idx] for i in range(len(tr))]\n",
        "        for i in range(len(tr_rewards)-1, -1, -1):\n",
        "          trs_R[-1] = tr_rewards[i] + (f ** do_discount[i]) * trs_R[-1]\n",
        "      log = {}\n",
        "      log[f'{wandb_prefix} size'] = size\n",
        "      log[f'{wandb_prefix}_eval/mean {f:.2f} discount '] = torch.Tensor(trs_R).mean()\n",
        "      log[f'{wandb_prefix}_eval/median {f:.2f} discount '] = torch.Tensor(trs_R).median()\n",
        "      # log[f'{wandb_prefix}_eval/95_quanile {f:.2f} discount'] = torch.Tensor(trs_R).quantile(q=0.95)\n",
        "      log[f'{wandb_prefix} Return by trjs {f:.2f} hist (previous epoch)'] = wandb.Histogram(np_histogram=np.histogram(trs_R, bins=40, ))\n",
        "#       log[f'{wandb_prefix} lengths hist'] = wandb.Histogram(np_histogram=np.histogram(tr_lengths, bins=30, ))\n",
        "      if verbose:\n",
        "          wandb.log(log.copy())\n",
        "      log['Returns'] = trs_R\n",
        "      log[f'{wandb_prefix} lengths'] = tr_lengths\n",
        "    return log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTYQZ1qL9gFa",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TW_VkrRNJeaV",
        "outputId": "344f0c51-bc82-473e-db51-e87400471bc4",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class Logger:\n",
        "  \"\"\"\n",
        "  Supporting class, to be expanded.\n",
        "  Stores logging methods and relevant data.\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "      self,\n",
        "      NN_setup,\n",
        "      batch_size=batch_size,\n",
        "      between_logs = 10,\n",
        "  ):\n",
        "    self.actor = NN_setup['actor']\n",
        "    self.actor_opt = NN_setup['actor_opt']\n",
        "    self.critic = NN_setup['critic']\n",
        "    self.critic_opt = NN_setup['critic_opt']\n",
        "    self.grad_a = None\n",
        "    self.grad_c = None\n",
        "    self.weight_a = None\n",
        "    self.weight_c = None\n",
        "    # self.running_grad_mean = torch.zeros(len([p for p in self.model.parameters() if p.requires_grad])).to(device)\n",
        "    # self.running_grad2_mean = torch.zeros(len([p for p in self.model.parameters() if p.requires_grad])).to(device)\n",
        "    self.log_gamma = torch.tensor(0.95).to(device)\n",
        "    self.between_logs = between_logs\n",
        "    self.timer = {}\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def link_models(self,):\n",
        "    self.grad_a = [p.grad.detach() for p in self.actor.parameters() if p.requires_grad]\n",
        "    self.weight_a = [p.detach() for p in self.actor.parameters()]\n",
        "    self.grad_c = [p.grad.detach() for p in self.critic.parameters() if p.requires_grad]\n",
        "    self.weight_c = [p.detach() for p in self.critic.parameters()]\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def list_norm(self, l, p=2):\n",
        "    n = 0\n",
        "    for t in l:\n",
        "      n += t.detach().norm(p) ** p\n",
        "    return n.item() ** (1/p)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def list_cos_dist(self, a, b):\n",
        "    a_norm = self.list_norm(a, 2)\n",
        "    b_norm = self.list_norm(b, 2)\n",
        "    product = sum([torch.dot(torch.flatten(a[i]), torch.flatten(b[i])).item() for i in range(len(a))])\n",
        "    return product/(a_norm*b_norm)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def on_list(self, a, b, operation):\n",
        "    assert len(a) == len(b), 'lists lengths differ'\n",
        "    return [operation(a[i], b[i]) for i in range(len(a))]\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def running_mean(self, a, b):\n",
        "    return [a[i].mul(self.log_gamma) + b[i].mul(1 - self.log_gamma) for i in range(len(a))]\n",
        "\n",
        "  # @torch.no_grad()\n",
        "  # def step(self):\n",
        "  #   self.running_grad_mean = self.running_mean(self.running_grad_mean, self.grad)\n",
        "  #   self.running_grad2_mean = self.running_mean(self.running_grad2_mean, [g**2 for g in self.grad])\n",
        "\n",
        "  # @torch.no_grad()\n",
        "  # def grad_stdev(self):\n",
        "  #   dev = [self.running_grad2_mean[i] - m**2 for i, m in enumerate(self.running_grad_mean)]\n",
        "  #   return [torch.sqrt(torch.maximum(torch.tensor(0), d)) for d in dev]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRSEdgzW9gFc",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "### Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NsLLouOFgHQZ",
        "outputId": "fcc2f3f8-9c56-4071-ab7d-d944048bdc62",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class NN_Trainer:\n",
        "  def __init__(\n",
        "      self,\n",
        "      NN_setup,\n",
        "      logger=None,\n",
        "      trajectories=None,\n",
        "      batch_size=batch_size,\n",
        "      n_batches=1000,\n",
        "      target_update_steps = 20,\n",
        "      td_gamma=td_gamma,\n",
        "      ):\n",
        "    self.n_batches = n_batches\n",
        "    self.batch_number = -1\n",
        "    self.td_gamma = td_gamma\n",
        "    self.clip_eps = 2e-1\n",
        "    self.actor = NN_setup['actor'].train()\n",
        "    self.actor_opt = NN_setup['actor_opt']\n",
        "    self.prev_actor = copy.deepcopy(self.actor).eval()\n",
        "    self.critic = NN_setup['critic'].train()\n",
        "    self.target_critic = copy.deepcopy(self.critic).eval()\n",
        "    self.critic_opt = NN_setup['critic_opt']\n",
        "    self.trajectories = trajectories\n",
        "    self.batch_size = batch_size\n",
        "    self.target_update_steps = target_update_steps\n",
        "    self.logger = logger\n",
        "    self.log = {}\n",
        "\n",
        "  def get_each_loss(self,\n",
        "               features,\n",
        "               features_next,\n",
        "               rewards,\n",
        "               Returns,\n",
        "               is_last,\n",
        "               queues,\n",
        "               bins,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Computes losses for actor, critic and exploration (loss_ent) within PPO algorithm.\n",
        "    Decisions were made to avoid python loops at all costs --\n",
        "    varying action space is not particularly batch-friendly.\n",
        "    \"\"\"\n",
        "    logger = self.logger\n",
        "    self.log['Returns mean'] = Returns.mean().item()\n",
        "    self.log['Return std'] = Returns.std().item()\n",
        "    self.log['rewards mean'] = rewards.mean().item()\n",
        "\n",
        "    t_logits = time()\n",
        "    logits = self.actor(queues).squeeze()\n",
        "    with torch.no_grad():\n",
        "      prev_logits = self.prev_actor(queues).squeeze() # can do once in epoch\n",
        "    logger.timer['logits'] += time()-t_logits\n",
        "\n",
        "    t_values = time()\n",
        "    values = self.critic(features).squeeze()\n",
        "    with torch.no_grad():\n",
        "      next_values = self.target_critic(features_next).squeeze()\n",
        "    logger.timer['values'] += time()-t_values\n",
        "\n",
        "    t_logits_chosen = time()\n",
        "    logits_chosen = self.actor(features_next).squeeze() # can export\n",
        "    with torch.no_grad():\n",
        "      prev_logits_chosen = self.prev_actor(features_next).squeeze() # can export\n",
        "    logger.timer['logits chosen'] += time()-t_logits_chosen\n",
        "\n",
        "    self.log['V-func mean'] = torch.mean(values.detach()).item()\n",
        "    self.log['V-func stdev'] = torch.std(values.detach()).item()\n",
        "    hist = wandb.Histogram(np_histogram=np.histogram(values.detach().to('cpu'), bins=40, ))\n",
        "    self.log['V-func hist'] = hist\n",
        "\n",
        "    # critic loss\n",
        "    TD = values - (rewards + next_values * self.td_gamma * (1-is_last))\n",
        "    MC = (values - Returns).abs().mean()/10\n",
        "    loss_c = (TD**2).mean() # + MC\n",
        "\n",
        "    self.log['TD loss'] = (TD**2).mean().item()\n",
        "    self.log['MC loss'] = MC.item()\n",
        "    # hist = wandb.Histogram(np_histogram=np.histogram(TD.detach().to('cpu'), bins=20, ))\n",
        "    # self.log['TD hist'] = hist\n",
        "\n",
        "    t_splitting = time()\n",
        "    split_sizes = list(bins[1:]-bins[:-1])\n",
        "    logits_by_state = torch.split(logits, split_sizes)\n",
        "    prev_logits_by_state = torch.split(prev_logits, split_sizes)\n",
        "    logger.timer['splitting'] += time()-t_splitting\n",
        "\n",
        "    t_padding = time()\n",
        "    logits_pad = torch.nn.utils.rnn.pad_sequence(logits_by_state,\n",
        "                                                 padding_value=-float('inf'),\n",
        "                                                 batch_first=True)\n",
        "    prev_logits_pad = torch.nn.utils.rnn.pad_sequence(prev_logits_by_state,\n",
        "                                                      padding_value=-float('inf'),\n",
        "                                                      batch_first=True)\n",
        "    logger.timer['padding'] += time()-t_padding\n",
        "\n",
        "\n",
        "    # entropy loss\n",
        "    t_entropy_loss = time()\n",
        "\n",
        "    probs_by_state = nn.functional.softmax(logits_pad, dim=-1)\n",
        "    entropies = - probs_by_state * torch.log(torch.max(torch.tensor(1e-40), probs_by_state))\n",
        "    entropy_by_state_reg = torch.sum(entropies, dim=-1) / torch.log(torch.hstack(split_sizes)+1).to(device)\n",
        "\n",
        "    loss_ent = -entropy_by_state_reg.mean()\n",
        "    logger.timer['entropy loss'] += time()-t_entropy_loss\n",
        "\n",
        "    # actor loss\n",
        "    t_actor_loss = time()\n",
        "\n",
        "    prev_probs_by_state = nn.functional.softmax(prev_logits_pad, dim=-1)\n",
        "    logsexp = torch.logsumexp(logits_pad, dim=-1)\n",
        "    prev_logsexp = torch.logsumexp(prev_logits_pad, dim=-1)\n",
        "    probs_chosen = (logits_chosen - logsexp).exp()\n",
        "    prev_probs_chosen = (prev_logits_chosen - prev_logsexp).exp()\n",
        "\n",
        "    ratios = (probs_chosen / (prev_probs_chosen.detach()+1e-9)).to(device)\n",
        "    clipped = torch.clip(ratios, min=1-self.clip_eps, max=1+self.clip_eps)\n",
        "    Adv = - (TD * (1-is_last)).detach()\n",
        "    loss_a = - torch.min(ratios*Adv, clipped*Adv).mean()\n",
        "    logger.timer['actor loss'] += time()-t_actor_loss\n",
        "\n",
        "    return loss_a, loss_c, loss_ent\n",
        "\n",
        "\n",
        "\n",
        "  def learn_new_policy(self, ):\n",
        "    \"\"\"\n",
        "    Implements one learning cycle over collected dataset.\n",
        "    \"\"\"\n",
        "    self.prev_actor = copy.deepcopy(self.actor).eval()\n",
        "    logger=self.logger\n",
        "    logger.timer = {'logits': 0,\n",
        "                    'values': 0,\n",
        "                    'logits chosen': 0,\n",
        "                    'entropy loss': 0,\n",
        "                    'actor loss': 0,\n",
        "                    'total loss': 0,\n",
        "                    'optimizers step': 0,\n",
        "                    'optimizers no step': 0,\n",
        "                    'slice (x2)': 0,\n",
        "                    'logsexp': 0,\n",
        "                    'sample batch': 0,\n",
        "                    'splitting': 0,\n",
        "                    'padding': 0,\n",
        "                    }\n",
        "    for i in trange(self.n_batches):\n",
        "      self.batch_number = i\n",
        "      if self.batch_number % self.target_update_steps == 0:\n",
        "        self.target_critic = copy.deepcopy(self.critic).eval()\n",
        "      t_total_loss = time()\n",
        "\n",
        "      t_sample_batch = time()\n",
        "      sampled_batch = self.trajectories.sample_batch(self.batch_size)\n",
        "      logger.timer['sample batch'] += time() - t_sample_batch\n",
        "\n",
        "      loss_a, loss_c, loss_ent = self.get_each_loss(*sampled_batch)\n",
        "      loss = loss_a + loss_c + loss_ent/50\n",
        "      logger.timer['total loss'] += time()-t_total_loss\n",
        "\n",
        "      self.critic_opt.zero_grad()\n",
        "      self.actor_opt.zero_grad()\n",
        "\n",
        "      t_optimizers_no_step = time()\n",
        "      loss.backward()\n",
        "      logger.timer['optimizers no step'] += time()-t_optimizers_no_step\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 20)\n",
        "      torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 20)\n",
        "      t_optimizers_step = time()\n",
        "      self.actor_opt.step()\n",
        "      self.critic_opt.step()\n",
        "      logger.timer['optimizers step'] += time()-t_optimizers_step\n",
        "\n",
        "      logger.link_models()\n",
        "\n",
        "      if self.batch_number % (logger.between_logs+1) == 0:\n",
        "        self.log.update({\n",
        "            'loss actor': loss_a.item(),\n",
        "            'loss critic': loss_c.item(),\n",
        "            'entropy by log(n)': loss_ent.item(),\n",
        "            'grad actor L2': logger.list_norm(logger.grad_a, 2),\n",
        "            'grad critic L2': logger.list_norm(logger.grad_c, 2),\n",
        "            'weight actor L2': logger.list_norm(logger.weight_a, 2),\n",
        "            'weight critic L2': logger.list_norm(logger.weight_c, 2),\n",
        "            # 'some_feature_mean': features[:, self.trajectories.feature_names2ids['some_feature']].mean()\n",
        "        })\n",
        "        wandb.log(self.log)\n",
        "        self.log = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX4jPlzD9gFd",
        "tags": []
      },
      "source": [
        "### Procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "Bi2uaButgHOM",
        "outputId": "ac74fdf1-0fb8-4613-b567-6b8b14dec89b",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.7 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/st-andrey-podivilov/Notebooks/wandb/run-20230728_115235-acoeoan9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/andrey_podivilov/PS%20PPO/runs/acoeoan9' target=\"_blank\">val %5==0</a></strong> to <a href='https://wandb.ai/andrey_podivilov/PS%20PPO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/andrey_podivilov/PS%20PPO' target=\"_blank\">https://wandb.ai/andrey_podivilov/PS%20PPO</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/andrey_podivilov/PS%20PPO/runs/acoeoan9' target=\"_blank\">https://wandb.ai/andrey_podivilov/PS%20PPO/runs/acoeoan9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/st-andrey-podivilov/.conda/envs/env1/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "247 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'queue max length, idx': (111, 4666), 'total number of states': 29145, 'number of traj-s': 574, 'number of validation traj-s': 111}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 1000/1000 [00:47<00:00, 21.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'logits': 0.7530734539031982, 'values': 0.5986373424530029, 'logits chosen': 0.5661764144897461, 'entropy loss': 0.5391933917999268, 'actor loss': 0.3875000476837158, 'total loss': 29.308507919311523, 'optimizers step': 0.7897698879241943, 'optimizers no step': 13.039979457855225, 'slice (x2)': 0, 'logsexp': 0, 'sample batch': 8.324571132659912, 'splitting': 7.892240524291992, 'padding': 6.2383856773376465}\n",
            "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "252 [main] INFO org.jooq.aR - \n",
            "                                      \n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@  @@@@  @@  @@    @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@    @@  @@  @@@@  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@  @  @  @@@@@@@@@@\n",
            "@@@@@@@@@@        @@        @@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@  @@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
            "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  Thank you for using jOOQ 3.14.16\n",
            "                                      \n"
          ]
        }
      ],
      "source": [
        "epochs = 30\n",
        "eval_conditions=[(lambda x: x%5==0)]\n",
        "td_gammas = [0.99]\n",
        "\n",
        "\n",
        "for (eval_condition, td_gamma) in zip(eval_conditions, td_gammas):\n",
        "    run = wandb.init(\n",
        "          project=\"PS PPO\",\n",
        "          name=f'val {inspect.getsourcelines(eval_condition)[0][0].split(\"x\")[-1].split(\")\")[0]}',\n",
        "          config={\n",
        "              'algorithm': 'PPO-clip',\n",
        "              'models': 'mlp',\n",
        "          }\n",
        "    )\n",
        "\n",
        "    # first we evaluate BFS heuristic (not to be confused with naive BFS)\n",
        "    # gather dataset\n",
        "    time_before = time()\n",
        "    os.system('rm -f ../Game_env/model.onnx')\n",
        "    os.system(jar_command)\n",
        "    print('BFS data gathering time:', time() - time_before)\n",
        "\n",
        "    trajectories = Trajectories(json_path,\n",
        "                                eval_condition=eval_condition,\n",
        "                                td_gamma=td_gamma,\n",
        "                                )\n",
        "    trajectories.evaluate_data()\n",
        "    trajectories.evaluate_data(wandb_prefix='train',\n",
        "                               eval_condition=(lambda x: not trajectories.eval_condition(x)),\n",
        "                               )\n",
        "\n",
        "    # then collect new json data file using randomly initialized policy neural network\n",
        "    actor, actor_opt = get_model_setup(use_FFM=True, wd=0.001)\n",
        "    critic, critic_opt = get_model_setup(use_FFM=True,)\n",
        "    logger = Logger(NN_setup={'actor': actor, 'actor_opt': actor_opt,\n",
        "                              'critic': critic, 'critic_opt': critic_opt,},\n",
        "                    batch_size=batch_size,\n",
        "                    between_logs = 50,\n",
        "                    )\n",
        "    trainer = NN_Trainer(NN_setup={'actor': actor, 'actor_opt': actor_opt,\n",
        "                                    'critic': critic, 'critic_opt': critic_opt,},\n",
        "                          logger=logger,\n",
        "                          trajectories=trajectories,\n",
        "                          batch_size=batch_size,\n",
        "                          )\n",
        "    trajectories.update_data_on_path(path='../Data/current_dataset.json', model=trainer.actor)\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # update dataset\n",
        "        trajectories = Trajectories(json_path,\n",
        "                                    eval_condition=eval_condition,\n",
        "                                    td_gamma=td_gamma,\n",
        "                                   )\n",
        "        print(trajectories.get_properties())\n",
        "        logger = Logger(NN_setup={'actor': actor, 'actor_opt': actor_opt,\n",
        "                                  'critic': critic, 'critic_opt': critic_opt,},\n",
        "                        batch_size=batch_size,\n",
        "                        between_logs = 50,\n",
        "                        )\n",
        "        trainer = NN_Trainer(NN_setup={'actor': actor, 'actor_opt': actor_opt,\n",
        "                                       'critic': critic, 'critic_opt': critic_opt,},\n",
        "                             logger=logger,\n",
        "                             trajectories=trajectories,\n",
        "                             batch_size=batch_size,\n",
        "                             n_batches=1000,\n",
        "                             td_gamma=td_gamma,\n",
        "                             )\n",
        "\n",
        "        trajectories.evaluate_data()\n",
        "        trajectories.evaluate_data(wandb_prefix='train',\n",
        "                                   eval_condition=(lambda x: not trajectories.eval_condition(x)),\n",
        "                                   )\n",
        "        trainer.learn_new_policy()\n",
        "        print(logger.timer)\n",
        "\n",
        "        wandb.log({'epoch': epoch,\n",
        "                })\n",
        "\n",
        "        time_before = time()\n",
        "        trajectories.update_data_on_path(path='../Data/current_dataset.json', model=trainer.actor)\n",
        "        print('Data gathering time: ', time()-time_before)\n",
        "\n",
        "    trajectories.update_data_on_path(path='../Data/current_dataset.json', model=trainer.actor)\n",
        "    trajectories.evaluate_data()\n",
        "\n",
        "    checkpoint = {\n",
        "    'actor': actor,\n",
        "    'critic':critic,\n",
        "    }\n",
        "    # torch.save(checkpoint, os.path.join(wandb.run.dir, f'mlp for TD multistep'))\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ1CwFlb9gFe"
      },
      "outputs": [],
      "source": [
        "# exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRN1NVVF9gFf",
        "tags": []
      },
      "source": [
        "### Side Utils and commented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3f_oNrr9gFf"
      },
      "outputs": [],
      "source": [
        "# def R_dif_max():\n",
        "#     TrsBFS = Trajectories(path='../Data/BFS_dataset.json', eval_condition=(lambda x:False))\n",
        "#     TrsNN = Trajectories(eval_condition=(lambda x:False))\n",
        "\n",
        "#     Returns_BFS = TrsBFS.evaluate_data(factors=[1], eval_condition=(lambda x: True), verbose=False)['Returns']\n",
        "#     Returns_NN = TrsNN.evaluate_data(factors=[1], eval_condition=(lambda x: True), verbose=False)['Returns']\n",
        "\n",
        "#     Returns_dif = torch.Tensor(Returns_BFS) - torch.Tensor(Returns_NN)\n",
        "#     max_idx = torch.argmax(Returns_dif)\n",
        "#     max_dif = Returns_dif[max_idx]\n",
        "#     assert TrsBFS.j_file['paths'][max_idx][2]==TrsNN.j_file['paths'][max_idx][2], 'wtf'\n",
        "#     return TrsNN.j_file['paths'][max_idx][2], TrsBFS.j_file['paths'][max_idx][2], max_idx, max_dif, Returns_BFS[max_idx], Returns_NN[max_idx], len(Returns_dif)\n",
        "\n",
        "# name_max, *a = R_dif_max()\n",
        "# R_dif_max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A2a3BefhAeDW"
      },
      "outputs": [],
      "source": [
        "#@title Fit a tree\n",
        "\n",
        "# Features, _, _, R, _ =  Trajectories(json_path).trs_tensors\n",
        "# r_tree.fit(Features.to('cpu'), R.to('cpu'))\n",
        "# R_prediction = r_tree.predict(Features.to('cpu'))\n",
        "# print(f'leaves: {r_tree.get_n_leaves()}, number of states: {Trajectories(json_path).n_sarsa_pairs}, depth: {totalr_tree.get_depth()}')\n",
        "# torch.mean((R.to('cpu') - torch.Tensor(R_prediction))**2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VTYQZ1qL9gFa",
        "tRSEdgzW9gFc",
        "mRN1NVVF9gFf"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env1_new",
      "language": "python",
      "name": "env1_new"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}