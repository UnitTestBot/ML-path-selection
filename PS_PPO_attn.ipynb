{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of PPO-clip for path selection for symbolic execution.\n",
    "Each epoch we communicate with jar-file for data gathering and wandb for logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6GWfxrs9gFR",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BAP5ws3ofyyY",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrey_podivilov\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%capture\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "\n",
    "from IPython.display import Javascript\n",
    "def resize_colab_cell():\n",
    "  display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 600})'))\n",
    "get_ipython().events.register('pre_run_cell', resize_colab_cell)\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import copy\n",
    "import inspect\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.onnx\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "from time import time\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "import math\n",
    "from operator import itemgetter \n",
    "\n",
    "\n",
    "# !pip install wandb\n",
    "import wandb\n",
    "\n",
    "# !pip install onnx==1.12\n",
    "# import onnx\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJbCP9HE9gFV",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Args (potentially immutable), login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "z90K8IBjpUBu",
    "outputId": "cda45a33-90d7-4e4f-90aa-b43648ac4507",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "max_nactions = 128\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "td_gamma=0.99\n",
    "json_path = '../Data/current_dataset.json'\n",
    "use_fork_discount = False\n",
    "batch_accumulation_steps = 1\n",
    "cuda_sync = False\n",
    "\n",
    "maybe_sync = torch.cuda.synchronize if cuda_sync else (lambda *args: None)\n",
    "jar_command = '/home/st-andrey-podivilov/java16/usr/lib/jvm/bellsoft-java16-amd64/bin/java -Dorg.jooq.no-logo=true -jar ../Game_env/usvm-jvm/build/libs/usvm-jvm-new.jar ../Game_env/jar_config.txt > ../Game_env/jar_log.txt'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndNrbEp79gFX",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Models, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "abh-k_ytgHWT",
    "outputId": "d7afca61-ea0b-4f4d-d851-95c77b7b2cc1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class FFM_layer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    wtf\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "      super().__init__()\n",
    "      assert input_dim%2 == 0, 'even input_dim is more convenient'\n",
    "      self.fourier_matrix = torch.nn.Linear(input_dim, int(input_dim), bias=False)\n",
    "      nn.init.normal_(\n",
    "          self.fourier_matrix.weight,\n",
    "          std=1/np.sqrt(input_dim),\n",
    "      )\n",
    "      self.fourier_matrix.weight.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "      pre = x # self.fourier_matrix(x)\n",
    "      s = torch.sin(pre)\n",
    "      c = torch.cos(pre)\n",
    "      return torch.cat([x,s,c], dim=-1)\n",
    "    \n",
    "  \n",
    "class Attn_model(torch.nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    dim_feedforward=256, \n",
    "    dropout=0.0,    \n",
    "    use_FFM=True,\n",
    "  ):\n",
    "    super(Attn_model, self).__init__()\n",
    "    self.emb = nn.Sequential(\n",
    "      nn.LazyLinear(512),\n",
    "      nn.ReLU(),\n",
    "      nn.LayerNorm(512),\n",
    "      FFM_layer(512) if use_FFM else nn.Identity(),\n",
    "      nn.LazyLinear(d_model),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "    self.attn_EncoderLayer0 = nn.TransformerEncoderLayer(d_model, n_heads, dim_feedforward, dropout, batch_first=True)\n",
    "    # self.attn_EncoderLayer1 = nn.TransformerEncoderLayer(d_model, n_heads, dim_feedforward, dropout, batch_first=True)\n",
    "    self.head = nn.Sequential(\n",
    "      nn.LazyLinear(1),\n",
    "    )\n",
    "    self.sfmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "  def forward(self, x, mask=None):\n",
    "    x = self.emb(x)\n",
    "    x = self.attn_EncoderLayer0(x, src_key_padding_mask=mask)\n",
    "    # x = self.attn_EncoderLayer1(x, src_key_padding_mask=mask)\n",
    "    x = self.head(x).squeeze(-1)\n",
    "    if mask is None:\n",
    "      return self.sfmax(x)\n",
    "    inf_mask = mask.float().masked_fill(mask==True, -float('inf'))    \n",
    "    x = self.sfmax(x + inf_mask)\n",
    "    return x\n",
    "    \n",
    "    \n",
    "def get_mlp_setup(use_FFM=False,\n",
    "                    lr = 3e-4,\n",
    "                    wd=0.1,\n",
    "                    ):\n",
    "    mlp = nn.Sequential(\n",
    "        nn.LazyLinear(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512,256),\n",
    "        nn.LayerNorm(256),\n",
    "        FFM_layer(256) if use_FFM else nn.Identity(),\n",
    "        nn.LazyLinear(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512,512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512,1),\n",
    "    ).to(device)\n",
    "    mlp_opt = torch.optim.AdamW(mlp.parameters(), lr=lr, weight_decay=wd, betas=(0.9, 0.999))\n",
    "    return mlp, mlp_opt\n",
    "  \n",
    "  \n",
    "def get_attn_setup():\n",
    "  attn_model = Attn_model().to(device)\n",
    "  opt = torch.optim.AdamW(attn_model.parameters(), lr=3e-4, weight_decay=3e-2,)\n",
    "  return attn_model, opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Logger:\n",
    "  \"\"\"\n",
    "  Supporting class, to be expanded.\n",
    "  Intended to store logging utils and relevant data.\n",
    "  \"\"\"\n",
    "  def __init__(\n",
    "      self,\n",
    "      between_logs = 32,\n",
    "  ):\n",
    "    self.grad_a = None\n",
    "    self.grad_c = None\n",
    "    self.weight_a = None\n",
    "    self.weight_c = None\n",
    "    self.log_gamma = torch.tensor(0.95).to(device)\n",
    "    self.between_logs = between_logs\n",
    "    self.timer = {}\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def link_models(self,):\n",
    "    self.grad_a = [p.grad.detach() for p in self.actor.parameters() if p.requires_grad]\n",
    "    self.weight_a = [p.detach() for p in self.actor.parameters()]\n",
    "    self.grad_c = [p.grad.detach() for p in self.critic.parameters() if p.requires_grad]\n",
    "    self.weight_c = [p.detach() for p in self.critic.parameters()]\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def list_norm(self, l, p=2):\n",
    "    n = 0\n",
    "    for t in l:\n",
    "      n += t.detach().norm(p) ** p\n",
    "    return n.item() ** (1/p)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def list_cos_dist(self, a, b):\n",
    "    a_norm = self.list_norm(a, 2)\n",
    "    b_norm = self.list_norm(b, 2)\n",
    "    product = sum([torch.dot(torch.flatten(a[i]), torch.flatten(b[i])).item() for i in range(len(a))])\n",
    "    return product/(a_norm*b_norm)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def on_list(self, a, b, operation):\n",
    "    assert len(a) == len(b), 'lists lengths differ'\n",
    "    return [operation(a[i], b[i]) for i in range(len(a))]\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def running_mean(self, a, b):\n",
    "    return [a[i].mul(self.log_gamma) + b[i].mul(1 - self.log_gamma) for i in range(len(a))]\n",
    "  \n",
    "  \n",
    "logger = Logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBhLnmrnFDAu",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ov4TPknTgHSy",
    "outputId": "56487d39-1ae0-4e7a-a179-728941e4d9bf",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Trajectories:\n",
    "  \"\"\"\n",
    "  Contains all kinds of data in a form of tensor.\n",
    "  realized_tensors are raw and derived features of visited states.\n",
    "  queues is a list of each states' actions features.\n",
    "  Action and state embeddings are effectively the same, fyi. \n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               td_gamma=td_gamma,\n",
    "               train_condition = (lambda tr: tr[0]%5!=0),\n",
    "              ):\n",
    "    self.train_condition = train_condition\n",
    "    self.td_gamma = td_gamma\n",
    "    self.j_file = None\n",
    "    self.feature_names, self.feature_names2ids = None, None\n",
    "    self.realized_tensors, self.queues = None, None # list of 7 train tensors f, f_n, r, R, is_last, queue_length, chosen_actions + list of queue tensors\n",
    "    self.sampled_ids_list = []\n",
    "    self.sampled_queue_lengths_list = []\n",
    "    \n",
    "  \n",
    "  def gather_n_store(self, model=None, json_path=json_path):\n",
    "    \"\"\"\n",
    "    Plays, collects trajectories into json, then transforms json to tensors\n",
    "    \"\"\"\n",
    "    self.update_data_on_path(model, json_path)\n",
    "    self.store_from_json(json_path)\n",
    "    \n",
    "    \n",
    "  def evaluate_val_train(self, verbose=True,):\n",
    "    \"\"\"\n",
    "    Evaluates val and train subset\n",
    "    \"\"\"\n",
    "    log_val = self.evaluate_data(eval_condition = (lambda tr: not self.train_condition(tr)),\n",
    "                                 wandb_prefix = 'val', \n",
    "                                 verbose=verbose,)\n",
    "    log_train = self.evaluate_data(eval_condition = self.train_condition,\n",
    "                                   wandb_prefix = 'train',\n",
    "                                   verbose=verbose,)\n",
    "    return log_val, log_train\n",
    "  \n",
    "  \n",
    "  def store_from_json(self, json_path=json_path):\n",
    "    \"\"\"\n",
    "    Extracts trajectories from json in a torch-friendly form\n",
    "    \"\"\"\n",
    "    self.j_file = json.load(open(json_path))\n",
    "    self.feature_names = self.j_file['scheme'][0]\n",
    "    self.feature_names2ids = {self.feature_names[i]:i for i in range(len(self.feature_names))}\n",
    "    self.realized_tensors, self.queues = self.j2torch(self.j_file)\n",
    "    \n",
    "  \n",
    "  def avg_code_length(self):\n",
    "    train_lines = 0\n",
    "    val_lines = 0\n",
    "    for tr in self.j_file['paths']:\n",
    "      if self.train_condition(tr):\n",
    "        train_lines += tr[3]\n",
    "      else:\n",
    "        val_lines += tr[3]\n",
    "    n_train_tr = self.get_properties()['number of traj-s']\n",
    "    n_val_tr = self.get_properties()['number of validation traj-s']\n",
    "    return train_lines/(n_train_tr-n_val_tr), val_lines/n_val_tr \n",
    "\n",
    "    \n",
    "  def j2torch(self, j_file):\n",
    "    \"\"\"\n",
    "    Transforms json to data tensors.\n",
    "    Queues are flipped for reasons related to batch truncation and padding. Actions numeration is changed accordingly.\n",
    "    \"\"\"\n",
    "    features, features_next, rewards, Returns, is_last, queue_lengths, chosen_actions, queues = [], [], [], [], [], [], [], []\n",
    "    chosenStId_idx = self.j_file['scheme'].index('chosenStateId')\n",
    "    rewards_idx = self.j_file['scheme'].index('reward')\n",
    "\n",
    "    for tr in self.j_file['paths']:\n",
    "      if not self.train_condition(tr):\n",
    "        continue\n",
    "      tr = tr[1]\n",
    "      tr_rewards = [tr[i][rewards_idx] for i in range(len(tr))][1:] + [0]\n",
    "      # we use features as a state emb and reward is what we get the next step\n",
    "      rewards += tr_rewards\n",
    "      do_discount = torch.ones(len(tr))\n",
    "      if use_fork_discount:\n",
    "        is_cfg_fork_idx = self.j_file['scheme'].index('is_cfg_fork')\n",
    "        do_discount = [tr[i][is_cfg_fork_idx] for i in range(len(tr))]\n",
    "      tr_Returns = self.tr_rewards_to_returns(tr_rewards, do_discount)\n",
    "      Returns += tr_Returns\n",
    "      \n",
    "      tr_chosen_actions = [tr[i][chosenStId_idx] for i in range(len(tr))][1:] + [0]\n",
    "      chosen_actions += tr_chosen_actions\n",
    "      \n",
    "      tr_features = [tr[i][0][tr[i][chosenStId_idx]] for i in range(len(tr))]\n",
    "      is_last += [0]*(len(tr_features)-1) + [1]\n",
    "      features += tr_features\n",
    "      features_next += tr_features[1:] + [[-1]*len(self.feature_names)]\n",
    "\n",
    "      tr_queues = [torch.Tensor(tr[i][0]).flip(dims=[0]) for i in range(len(tr))][1:] + [torch.zeros_like(torch.Tensor([tr[0][0][0]]))]\n",
    "      tr_queue_lengths = [len(q) for q in tr_queues]\n",
    "      queues += tr_queues\n",
    "      queue_lengths += tr_queue_lengths\n",
    "    rewards = torch.Tensor(rewards).to(device)\n",
    "    features = torch.Tensor(features).to(device)\n",
    "    features_next = torch.Tensor(features_next).to(device)\n",
    "    Returns = torch.Tensor(Returns).to(device)\n",
    "    is_last = torch.Tensor(is_last).to(device)\n",
    "    queue_lengths = torch.LongTensor(queue_lengths).to(device)\n",
    "    # we flip queue and numeration of actions\n",
    "    chosen_actions = queue_lengths - torch.LongTensor(chosen_actions).to(device) - 1 \n",
    "    return [features, features_next, rewards, Returns, is_last, queue_lengths, chosen_actions], queues\n",
    "  \n",
    "\n",
    "  def n_train_states(self):\n",
    "    return len(self.realized_tensors[-1])\n",
    "\n",
    "  \n",
    "  def get_properties(self):\n",
    "    queue_lengths = np.array([q.shape[0] for q in self.queues])\n",
    "    longest_queue_ids = np.argmax(queue_lengths)\n",
    "    tr_lengths = np.array([len(tr[1]) for tr in self.j_file['paths']])\n",
    "    prop = {\n",
    "            'traj_length mean, median, max': (f'{tr_lengths.mean():.2f}', np.median(tr_lengths), tr_lengths.max()),\n",
    "            'queue max length, idx': (self.queues[longest_queue_ids].shape[0], longest_queue_ids),\n",
    "            'number of train states': len(self.realized_tensors[-1]),\n",
    "            'number of traj-s': len(self.j_file['paths']),\n",
    "            'number of validation traj-s': sum([not self.train_condition(tr) for tr in self.j_file['paths']]),\n",
    "           }\n",
    "    return prop\n",
    "  \n",
    "\n",
    "  def tr_rewards_to_returns(self, tr_rewards, do_discount):\n",
    "    tr_R = [0]*(len(tr_rewards))\n",
    "    for i in range(len(tr_rewards)-2, -1, -1):\n",
    "        tr_R[i] = tr_rewards[i] + (self.td_gamma**do_discount[i]) * tr_R[i+1]\n",
    "    return tr_R\n",
    "  \n",
    "  \n",
    "  def sample_ids(self, batch_size=batch_size):\n",
    "    \"\"\"\n",
    "    Assembles ids for several consiquent batches based on queues lengths.\n",
    "    \"\"\"\n",
    "    if len(self.sampled_ids_list) == 0:\n",
    "      sampled_ids = torch.LongTensor(random.choice(self.n_train_states(), size=batch_size*batch_accumulation_steps)).to(device)\n",
    "      sampled_queue_lengths = self.realized_tensors[5][sampled_ids]\n",
    "      sampled_queue_lengths, sorting_ids = torch.sort(sampled_queue_lengths)\n",
    "      sampled_ids = sampled_ids[sorting_ids]\n",
    "      self.sampled_ids_list = list(torch.split(sampled_ids, batch_size))\n",
    "      self.sampled_queue_lengths_list = list(torch.split(sampled_queue_lengths, batch_size))\n",
    "    return self.sampled_ids_list.pop(0), self.sampled_queue_lengths_list.pop(0)\n",
    "      \n",
    "\n",
    "  def sample_batch(self, batch_size=batch_size):\n",
    "    start, end = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "  \n",
    "    ids, queue_lengths = self.sample_ids(batch_size=batch_size)\n",
    "    \n",
    "    end.record()    \n",
    "    maybe_sync()\n",
    "    logger.timer['sample ids'] += start.elapsed_time(end)/1000\n",
    "    \n",
    "    sampled_realized = [t[ids] for t in self.realized_tensors]\n",
    "    \n",
    "    sampled_queues = itemgetter(*list(ids))(self.queues)\n",
    "    padded_length = torch.minimum(queue_lengths.max(), torch.tensor(max_nactions))\n",
    "    queues_tensor = torch.zeros(batch_size, padded_length, len(self.feature_names)).to(device)\n",
    "    pad_mask = torch.ones(batch_size, padded_length).to(device)\n",
    "    \n",
    "    start, end = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    \n",
    "    for i, q in enumerate(sampled_queues):\n",
    "      l = min(q.shape[0], padded_length)\n",
    "      queues_tensor[i, 0 : l, :] = q[: l, :]\n",
    "      pad_mask[i, 0 : l] = 0\n",
    "    pad_mask = pad_mask.bool()\n",
    "    \n",
    "    end.record()    \n",
    "    maybe_sync()\n",
    "    logger.timer['sample inside loop'] += start.elapsed_time(end)/1000\n",
    "\n",
    "    return *sampled_realized, queues_tensor, pad_mask\n",
    "  \n",
    "\n",
    "  def update_data_on_path(self, model=None, path=json_path):\n",
    "    \"\"\"\n",
    "    Communication with jar file on a server.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "      # use default heuristics\n",
    "      time_before = time()\n",
    "      os.system('rm -f ../Game_env/model.onnx')\n",
    "      os.system(jar_command)\n",
    "      print('BFS data gathering time:', time() - time_before)\n",
    "    else:\n",
    "      x = torch.randn(1, 1, len(self.feature_names), requires_grad=True).to(device)\n",
    "      torch_model = model.eval()\n",
    "      torch_out = torch_model(x)\n",
    "      torch.onnx.export(torch_model,\n",
    "                        x,\n",
    "                        '../Game_env/model.onnx',\n",
    "                        opset_version=15,\n",
    "                        export_params=True,\n",
    "                        input_names = ['input'],   # the model's input names\n",
    "                        output_names = ['output'],\n",
    "                        dynamic_axes={'input' : {0 : 'batch_size',\n",
    "                                                 1 : 'n_actions',\n",
    "                                                },    # variable length axes\n",
    "                                      'output' : {0 : 'batch_size',\n",
    "                                                  1 : 'n_actions',\n",
    "                                                },\n",
    "                                      },\n",
    "                        )\n",
    "      os.system(jar_command)\n",
    "  \n",
    "    \n",
    "  def evaluate_data(self,\n",
    "           eval_condition = None,\n",
    "           wandb_prefix = 'val',\n",
    "           verbose=True,\n",
    "           factors=torch.Tensor([1, 0.99]),\n",
    "           ):\n",
    "    if eval_condition is None:\n",
    "        eval_condition = (lambda tr: not self.train_condition(tr))\n",
    "    rewards_idx = self.j_file['scheme'].index('reward')\n",
    "    for f in factors:\n",
    "      size = 0\n",
    "      tr_lengths = []\n",
    "      trs_R = []\n",
    "      for tr in self.j_file['paths']:\n",
    "        if not eval_condition(tr):\n",
    "          continue\n",
    "        tr=tr[1]\n",
    "        size += len(tr)\n",
    "        tr_lengths += [len(tr)]\n",
    "        # rewards list is not shifted, has a different purpose this time\n",
    "        tr_rewards = [tr[i][rewards_idx] for i in range(len(tr))]\n",
    "        trs_R += [0]\n",
    "        do_discount = torch.Tensor([1]*len(tr))\n",
    "        if use_fork_discount:\n",
    "          is_cfg_fork_idx = self.j_file['scheme'].index('is_cfg_fork')\n",
    "          do_discount = [tr[i][is_cfg_fork_idx] for i in range(len(tr))]\n",
    "        for i in range(len(tr_rewards)-1, -1, -1):\n",
    "          trs_R[-1] = tr_rewards[i] + (f ** do_discount[i]) * trs_R[-1]\n",
    "      log = {}\n",
    "      log[f'{wandb_prefix} size'] = size\n",
    "      log[f'{wandb_prefix}_eval/mean {f:.2f} discount '] = torch.Tensor(trs_R).mean()\n",
    "      log[f'{wandb_prefix}_eval/median {f:.2f} discount '] = torch.Tensor(trs_R).median()\n",
    "      log[f'{wandb_prefix} Return by trjs {f:.2f} (previous epoch)'] = wandb.Histogram(np_histogram=np.histogram(trs_R, bins=30, ))\n",
    "      # log[f'{wandb_prefix} lengths hist'] = wandb.Histogram(np_histogram=np.histogram(tr_lengths, bins=30, ))\n",
    "      log['code length train mean'], log['code length val mean'] = self.avg_code_length()\n",
    "      if verbose:\n",
    "          wandb.log(log.copy())\n",
    "      log['Returns'] = trs_R\n",
    "      log[f'{wandb_prefix} lengths'] = tr_lengths\n",
    "    return log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRSEdgzW9gFc",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "NsLLouOFgHQZ",
    "outputId": "fcc2f3f8-9c56-4071-ab7d-d944048bdc62",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NN_Trainer:\n",
    "  def __init__(\n",
    "      self,\n",
    "      NN_setup,\n",
    "      trajectories=None,\n",
    "      batch_size=batch_size,\n",
    "      n_batches=1000,\n",
    "      target_update_steps = 15,\n",
    "      td_gamma=td_gamma,\n",
    "      clip_eps = 3e-1\n",
    "      ):\n",
    "    self.n_batches = n_batches\n",
    "    self.batch_number = -1\n",
    "    self.td_gamma = td_gamma\n",
    "    self.clip_eps = clip_eps\n",
    "    self.actor = NN_setup['actor'].train()\n",
    "    self.actor_opt = NN_setup['actor_opt']\n",
    "    self.prev_actor = copy.deepcopy(self.actor).eval()\n",
    "    self.critic = NN_setup['critic'].train()\n",
    "    self.target_critic = copy.deepcopy(self.critic).eval()\n",
    "    self.critic_opt = NN_setup['critic_opt']\n",
    "    self.trajectories = trajectories\n",
    "    self.batch_size = batch_size\n",
    "    self.target_update_steps = target_update_steps\n",
    "    self.log = {}\n",
    "\n",
    "  def get_each_loss(self,\n",
    "               features,\n",
    "               features_next,\n",
    "               rewards,\n",
    "               Returns,\n",
    "               is_last,\n",
    "               queue_lengths,\n",
    "               chosen_actions,\n",
    "               queues_tensor, \n",
    "               pad_mask,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Computes losses for actor, critic and exploration (loss_ent) within PPO algorithm.\n",
    "    Decisions were made to avoid python loops at all costs -- \n",
    "    varying action space is not particularly batch-friendly.    \n",
    "    \"\"\"\n",
    "    self.log['Returns mean'] = Returns.mean().item()\n",
    "    # self.log['Return std'] = Returns.std().item()\n",
    "    self.log['rewards mean'] = rewards.mean().item()\n",
    "    self.log['queue length max'] = queue_lengths.max().item()\n",
    "        \n",
    "        \n",
    "    start, end = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    \n",
    "    probs = self.actor(queues_tensor, mask=pad_mask)\n",
    "    with torch.no_grad():\n",
    "      prev_probs = self.prev_actor(queues_tensor, mask=pad_mask) # can do once in epoch\n",
    "    self.log['max prob mean'] = probs.max(dim=-1).values.mean().item()\n",
    "    self.log['40 max prob quantile'] = torch.quantile(probs.max(dim=-1).values, 0.4).item()\n",
    "    \n",
    "    end.record()    \n",
    "    maybe_sync()\n",
    "    logger.timer['probs'] += start.elapsed_time(end)/1000\n",
    "\n",
    "    start, end = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    \n",
    "    values = self.critic(features).squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "      next_values = self.target_critic(features_next).squeeze(-1)\n",
    "    \n",
    "    end.record()    \n",
    "    maybe_sync()\n",
    "    logger.timer['values'] += start.elapsed_time(end)/1000\n",
    "        \n",
    "    t_hist = time()\n",
    "    self.log['V-func mean'] = torch.mean(values.detach()).item()\n",
    "    self.log['V-func stdev'] = torch.std(values.detach()).item()\n",
    "    # hist = wandb.Histogram(np_histogram=np.histogram(values.detach().to('cpu'), bins=40, ))\n",
    "    # self.log['V-func hist'] = hist\n",
    "    logger.timer['hist'] += time() - t_hist\n",
    "\n",
    "    # critic loss\n",
    "    t_critic_loss = time()\n",
    "    TD = values - (rewards + next_values * self.td_gamma * (1-is_last))\n",
    "    MC = (values - Returns).abs().mean()/10\n",
    "    loss_c = (TD**2).mean().sqrt() # + MC\n",
    "    logger.timer['critic loss'] += time()-t_critic_loss\n",
    "\n",
    "    self.log['TD loss'] = (TD**2).mean().sqrt().item()\n",
    "    self.log['MC loss'] = MC.item()\n",
    "    # hist = wandb.Histogram(np_histogram=np.histogram(TD.detach().to('cpu'), bins=20, ))\n",
    "    # self.log['TD hist'] = hist\n",
    "        \n",
    "    # entropy loss\n",
    "    t_entropy_loss = time()\n",
    "    \n",
    "    entropies = - probs * torch.log(torch.max(torch.tensor(1e-40), probs))\n",
    "    entropy_by_state_reg = torch.sum(entropies, dim=-1) / torch.log(torch.minimum(queue_lengths+1, torch.tensor(max_nactions+1))).to(device)\n",
    "    \n",
    "    loss_ent = -entropy_by_state_reg.mean()\n",
    "        \n",
    "    logger.timer['entropy loss'] += time()-t_entropy_loss\n",
    "    \n",
    "    # actor loss\n",
    "    start, end = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    \n",
    "    probs_chosen = probs[get_ass_dope_ids(chosen_actions)].squeeze(-1)\n",
    "    prev_probs_chosen = prev_probs[get_ass_dope_ids(chosen_actions)].squeeze(-1)\n",
    "        \n",
    "    ratios = (probs_chosen / (prev_probs_chosen.detach()+1e-9)).to(device)\n",
    "    clipped = torch.clip(ratios, min=1-self.clip_eps, max=1+self.clip_eps)\n",
    "    Adv = - (TD * (1-is_last)).detach()\n",
    "    loss_a = - torch.min(ratios*Adv, clipped*Adv).mean()\n",
    "        \n",
    "    self.log['clipped to all'] = (clipped*Adv < ratios*Adv).long().sum().item() / ratios.numel()\n",
    "    self.log['ratios mean'] = ratios.mean().item()\n",
    "    \n",
    "    end.record()    \n",
    "    maybe_sync()\n",
    "    logger.timer['actor loss'] += start.elapsed_time(end)/1000\n",
    "    \n",
    "    return loss_a, loss_c, loss_ent\n",
    "\n",
    "\n",
    "\n",
    "  def learn_new_policy(self, ):\n",
    "    \"\"\"\n",
    "    Implements one learning cycle over collected dataset.\n",
    "    \"\"\"\n",
    "    self.prev_actor = copy.deepcopy(self.actor).eval()\n",
    "    logger.timer = {'probs': 0,\n",
    "                    'values': 0,\n",
    "                    'hist': 0,\n",
    "                    'critic loss': 0,\n",
    "                    'entropy loss': 0,\n",
    "                    'actor loss': 0,\n",
    "                    'total loss': 0,\n",
    "                    'optimizers step': 0,\n",
    "                    'loss.backward': 0,\n",
    "                    'sample batch': 0,\n",
    "                    'sample ids': 0,\n",
    "                    'sample inside loop': 0,\n",
    "                    }\n",
    "    for i in trange(self.n_batches):\n",
    "      self.batch_number = i\n",
    "      if self.batch_number % self.target_update_steps == 0:\n",
    "        self.target_critic = copy.deepcopy(self.critic).eval()\n",
    "      \n",
    "      start, end = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "      start.record()\n",
    "      \n",
    "      sampled_batch = self.trajectories.sample_batch(self.batch_size)\n",
    "      \n",
    "      end.record()    \n",
    "      maybe_sync()\n",
    "      logger.timer['sample batch'] += start.elapsed_time(end)/1000\n",
    "      \n",
    "      start, end = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "      start.record()      \n",
    "      \n",
    "      loss_a, loss_c, loss_ent = self.get_each_loss(*sampled_batch)\n",
    "      loss = (loss_a + loss_c + loss_ent/50) / batch_accumulation_steps\n",
    "\n",
    "      end.record()    \n",
    "      maybe_sync()\n",
    "      logger.timer['total loss'] += start.elapsed_time(end)/1000\n",
    "      \n",
    "      start, end = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "      start.record()   \n",
    "      \n",
    "      loss.backward()\n",
    "      \n",
    "      end.record()    \n",
    "      maybe_sync()\n",
    "      logger.timer['loss.backward'] += start.elapsed_time(end)/1000\n",
    "      \n",
    "      if self.batch_number % batch_accumulation_steps == 0:\n",
    "        t_optimizers_step = time()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 30)\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 30)\n",
    "        self.actor_opt.step()\n",
    "        self.critic_opt.step()\n",
    "        \n",
    "        self.log.update({\n",
    "            'grad actor L2': logger.list_norm([p.grad for p in self.actor.parameters() if p.requires_grad], 2),\n",
    "            'grad critic L2': logger.list_norm([p.grad for p in self.critic.parameters() if p.requires_grad], 2),\n",
    "        })\n",
    "        self.critic_opt.zero_grad()\n",
    "        self.actor_opt.zero_grad()\n",
    "        logger.timer['optimizers step'] += time()-t_optimizers_step\n",
    "        \n",
    "\n",
    "      if self.batch_number % (logger.between_logs+1) == 0:\n",
    "        self.log.update({\n",
    "            'loss actor': loss_a.item(),\n",
    "            'loss critic': loss_c.item(),\n",
    "            '-entropy by log(n)': loss_ent.item(),\n",
    "            'weight actor L2': logger.list_norm([p for p in self.actor.parameters() if p.requires_grad], 2),\n",
    "            'weight critic L2': logger.list_norm([p for p in self.critic.parameters() if p.requires_grad], 2),\n",
    "        })\n",
    "        wandb.log(self.log)\n",
    "        self.log = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_ass_dope_ids(l):\n",
    "  l = l.long()\n",
    "  if len(l.shape) == 1:\n",
    "    l = l[:, None]\n",
    "  ind = torch.LongTensor(np.indices(l.shape))\n",
    "  ind[-1] = l\n",
    "  return tuple(ind)\n",
    "\n",
    "def consumption_percent(epoch):\n",
    "  return 100 # 40 if (epoch<30) else 100\n",
    "\n",
    "def get_clip_eps(epoch):\n",
    "  return 0.5 if (epoch<5) else 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX4jPlzD9gFd",
    "tags": []
   },
   "source": [
    "### Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "Bi2uaButgHOM",
    "outputId": "ac74fdf1-0fb8-4613-b567-6b8b14dec89b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "google.colab.output.setIframeHeight(0, true, {maxHeight: 600})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/st-andrey-podivilov/Notebooks/wandb/run-20230804_093043-7lu0tue9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andrey_podivilov/delete/runs/7lu0tue9' target=\"_blank\">consumption, val h%5=0</a></strong> to <a href='https://wandb.ai/andrey_podivilov/delete' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andrey_podivilov/delete' target=\"_blank\">https://wandb.ai/andrey_podivilov/delete</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andrey_podivilov/delete/runs/7lu0tue9' target=\"_blank\">https://wandb.ai/andrey_podivilov/delete/runs/7lu0tue9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFS data gathering time: 167.7209391593933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st-andrey-podivilov/.conda/envs/envatt/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attn data gathering+storing:  154.76415181159973\n",
      "{'traj_length mean, median, max': ('58.95', 11.0, 1501), 'queue max length, idx': (243, 31714), 'number of train states': 35956, 'number of traj-s': 735, 'number of validation traj-s': 136}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:54<00:00, 18.34it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "train_condition, message = (lambda tr: tr[0]%5!=1), 'h%5=0'\n",
    "\n",
    "for _ in [0]:\n",
    "    run = wandb.init(\n",
    "          project=\"delete\",\n",
    "          name=f'consumption, val {message}',\n",
    "          config={\n",
    "              'algorithm': 'PPO-clip',\n",
    "              'models': 'mlp + attn',\n",
    "          }\n",
    "    )\n",
    "\n",
    "    # first we evaluate BFS heuristic (not to be confused with naive BFS)\n",
    "    \n",
    "    trajectories = Trajectories(train_condition=train_condition)\n",
    "\n",
    "    \n",
    "    with open('../Game_env/jar_config.txt', 'w') as jar_config:\n",
    "      jar_config.write(json.dumps({'postprocessing': 'None', \n",
    "                                   'dataConsumption': consumption_percent(100),\n",
    "                                   'maxAttentionLength': max_nactions,\n",
    "                                   'inputShape': [-1, -1, -1]}))\n",
    "    \n",
    "    trajectories.gather_n_store()\n",
    "\n",
    "    trajectories.evaluate_val_train()\n",
    "\n",
    "    # then collect new json data file using randomly initialized policy neural network\n",
    "    actor, actor_opt = get_attn_setup()\n",
    "    critic, critic_opt = get_mlp_setup(use_FFM=True, wd=0.01)\n",
    "    \n",
    "    time0 = time()\n",
    "    with open('../Game_env/jar_config.txt', 'w') as jar_config:\n",
    "      jar_config.write(json.dumps({'postprocessing': 'None', \n",
    "                                   'dataConsumption': consumption_percent(0),\n",
    "                                   'maxAttentionLength': max_nactions,\n",
    "                                   'inputShape': [1, -1, len(trajectories.feature_names)]}))\n",
    "    trajectories.gather_n_store(model=actor)\n",
    "    print('Attn data gathering+storing: ', time()-time0)\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        with open('../Game_env/jar_config.txt', 'w') as jar_config:\n",
    "          jar_config.write(json.dumps({'postprocessing': 'None', \n",
    "                                       'dataConsumption': consumption_percent(epoch),\n",
    "                                       'maxAttentionLength': max_nactions,\n",
    "                                       'inputShape': [1, -1, len(trajectories.feature_names)]}))\n",
    "        print(trajectories.get_properties())\n",
    "        \n",
    "        trainer = NN_Trainer(NN_setup={'actor': actor, 'actor_opt': actor_opt,\n",
    "                                       'critic': critic, 'critic_opt': critic_opt,},\n",
    "                             trajectories=trajectories,\n",
    "                             n_batches=int(1000 * consumption_percent(epoch)/100),\n",
    "                             clip_eps = get_clip_eps(epoch),\n",
    "                             )\n",
    "        trajectories.evaluate_val_train()\n",
    "        trainer.learn_new_policy()\n",
    "\n",
    "        wandb.log({'epoch': epoch,\n",
    "                })\n",
    "\n",
    "        time_before = time()\n",
    "        trajectories.gather_n_store(model=actor)\n",
    "        print('Data gathering time: ', time()-time_before)\n",
    "\n",
    "    trajectories.evaluate_val_train()\n",
    "\n",
    "    checkpoint = {\n",
    "    'actor': actor,\n",
    "    'critic':critic,\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(wandb.run.dir, f'actor, critic'))\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJ1CwFlb9gFe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRN1NVVF9gFf",
    "tags": []
   },
   "source": [
    "### Side Utils and commented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3f_oNrr9gFf",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def R_dif_max():\n",
    "#     TrsBFS = Trajectories(path='../Data/BFS_dataset.json', eval_condition=(lambda x:False))\n",
    "#     TrsNN = Trajectories(eval_condition=(lambda x:False))\n",
    "\n",
    "#     Returns_BFS = TrsBFS.evaluate_data(factors=[1], eval_condition=(lambda x: True), verbose=False)['Returns']\n",
    "#     Returns_NN = TrsNN.evaluate_data(factors=[1], eval_condition=(lambda x: True), verbose=False)['Returns']\n",
    "\n",
    "#     Returns_dif = torch.Tensor(Returns_BFS) - torch.Tensor(Returns_NN)\n",
    "#     max_idx = torch.argmax(Returns_dif)\n",
    "#     max_dif = Returns_dif[max_idx]\n",
    "#     assert TrsBFS.j_file['paths'][max_idx][2]==TrsNN.j_file['paths'][max_idx][2], 'wtf'\n",
    "#     return TrsNN.j_file['paths'][max_idx][2], TrsBFS.j_file['paths'][max_idx][2], max_idx, max_dif, Returns_BFS[max_idx], Returns_NN[max_idx], len(Returns_dif)\n",
    "\n",
    "# name_max, *a = R_dif_max()\n",
    "# R_dif_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "A2a3BefhAeDW",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Fit a tree\n",
    "\n",
    "\n",
    "# to check features' strength\n",
    "# r_tree = tree.DecisionTreeRegressor(max_depth=1000, )\n",
    "\n",
    "# Features, _, _, R, _ =  Trajectories(json_path).trs_tensors\n",
    "# r_tree.fit(Features.to('cpu'), R.to('cpu'))\n",
    "# R_prediction = r_tree.predict(Features.to('cpu'))\n",
    "# print(f'leaves: {r_tree.get_n_leaves()}, number of states: {Trajectories(json_path).n_sarsa_pairs}, depth: {totalr_tree.get_depth()}')\n",
    "# torch.mean((R.to('cpu') - torch.Tensor(R_prediction))**2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VTYQZ1qL9gFa",
    "tRSEdgzW9gFc",
    "mRN1NVVF9gFf"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-envatt] *",
   "language": "python",
   "name": "conda-env-.conda-envatt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
